# -*- coding: utf-8 -*-
"""LLM_Finetuning.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1m1Zv1hPz3vJ6C-LSmMg59xwk12k3Yng-
"""

!pip install -q -U bitsandbytes transformers peft accelerate datasets scipy einops evaluate trl rouge_score

!pip install GPUtil

import torch
import GPUtil
import os

#Loading the required libraries
!pip uninstall -y pyarrow datasets
!pip install pyarrow datasets
from datasets import load_dataset
from transformers import AutoModelForCausalLM,AutoTokenizer,BitsAndBytesConfig,HfArgumentParser,AutoTokenizer,TrainingArguments,Trainer,GenerationConfig
from tqdm import tqdm
from trl import SFTTrainer
import torch
import time
import pandas as pd
import numpy as np
from huggingface_hub import interpreter_login
interpreter_login()

from datasets import load_dataset
ds = load_dataset("ShenLab/MentalChat16K")

ds['train'][0]

ds=ds.remove_columns('instruction')

ds=ds["train"].train_test_split(test_size=0.2, seed=42)  # 80% train, 20% temp
ds["validation"] = ds["test"].train_test_split(test_size=0.5, seed=42)["train"]
ds["test"] = ds["test"].train_test_split(test_size=0.5, seed=42)["test"]
print(ds)

ds['train'].column_names[0:]

#Create Bitsandbytes configuration
compute_dtype=getattr(torch,"float16")
bnb_config=BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_quant_type='nf4',
    bnb_4bit_compute_dtype=compute_dtype,
    bnb_4bit_use_double_quant=False
)

#Loading the Pre-Trained model
model_name='microsoft/phi-2'
device_map={"":0}
original_model=AutoModelForCausalLM.from_pretrained(
    model_name,
    device_map=device_map,
    quantization_config=bnb_config,
    trust_remote_code=True,
    token=True
)

#Tokenization
tokenizer=AutoTokenizer.from_pretrained(model_name,trust_remote_code=True,padding_side="left",)
tokenizer.pad_token=tokenizer.eos_token

print(len(ds["train"]))

import torch
def gen(model, prompt, max_new_tokens=100, temperature=0.7, top_p=0.9, do_sample=True):
    """
    Generate text from a causal LM in zero-shot mode.
    Args:
        model: Hugging Face causal LM (can be 4-bit/LoRA)
        prompt: string, the input text
        max_new_tokens: int, max tokens to generate
        temperature: float, controls randomness
        top_p: float,nucleus sampling
        do_sample: bool, whether to sample or take greedy output.
    Returns:
        List of generated text strings.
    """
    # Move model to GPU if not already
    device = next(model.parameters()).device
    # Tokenize input
    inputs = tokenizer(prompt, return_tensors="pt").to(device)
    # Generate output
    with torch.no_grad():
        output_ids = model.generate(
            **inputs,
            max_new_tokens=max_new_tokens,
            do_sample=do_sample,
            temperature=temperature,
            top_p=top_p,
            pad_token_id=tokenizer.pad_token_id  # avoid errors if prompt shorter than max length
        )
    # Decode tokens to text
    text = tokenizer.batch_decode(output_ids, skip_special_tokens=True)
    return text

# Commented out IPython magic to ensure Python compatibility.
# #Test the Model with Zero Shot Inferencing
# %%time
# from transformers import set_seed
# seed = 42
# set_seed(seed)
# index = 1
# prompt = ds['test'][index]['input']
# summary = ds['test'][index]['output']
# formatted_prompt = f"Instruct: Summarize the following conversation.\n{prompt}\nOutput:\n"
# res = gen(original_model,formatted_prompt,100,)
# #print(res[0])
# output = res[0].split('Output:\n')[1]
# dash_line = '-'.join('' for x in range(100))
# print(dash_line)
# print(f'INPUT PROMPT:\n{formatted_prompt}')
# print(dash_line)
# print(f'BASELINE HUMAN SUMMARY:\n{summary}\n')
# print(dash_line)
# print(f'MODEL GENERATION - ZERO SHOT:\n{output}')

#preprocessing a daatset
def create_prompt_formats(sample):
    """
    Format various fields of the sample ('instruction','output')
    Then concatenate them using two newline characters
    :param sample: Sample dictionnary
    """
    INTRO_BLURB = "Below is an instruction that describes a task. Write a response that appropriately completes the request."
    INSTRUCTION_KEY = "### Instruct: Summarize the below conversation."
    RESPONSE_KEY = "### Output:"
    END_KEY = "### End"

    blurb = f"\n{INTRO_BLURB}"
    instruction = f"{INSTRUCTION_KEY}"
    input_context = f"{sample['input']}" if sample["input"] else None
    response = f"{RESPONSE_KEY}\n{sample['output']}"
    end = f"{END_KEY}"

    parts = [part for part in [blurb, instruction, input_context, response, end] if part]

    formatted_prompt = "\n\n".join(parts)
    sample["text"] = formatted_prompt

    return sample

from functools import partial

# This function finds the maximum number of tokens a model can handle at a time.
# If it can’t find it, it sets it to 1024 by default.
def get_max_length(model):
    conf = model.config
    max_length = None
    for length_setting in ["n_positions", "max_position_embeddings", "seq_length"]:
        max_length = getattr(model.config, length_setting, None)
        if max_length:
            print(f"Found max length: {max_length}")
            break
    if not max_length:
        max_length = 1024
        print(f"Using default max length: {max_length}")
    return max_length
# Tokenizing the batch of text
def preprocess_batch(batch, tokenizer, max_length):
    """
    Tokenizing a batch
    """
    return tokenizer(
        batch["text"],
        max_length=max_length,
        truncation=True,
    )

def preprocess_dataset(tokenizer: AutoTokenizer, max_length: int,seed, dataset):
    """Format & tokenize it so it is ready for training
    :param tokenizer (AutoTokenizer): Model Tokenizer
    :param max_length (int): Maximum number of tokens to emit from tokenizer
    """
    # Add prompt to each sample
    print("Preprocessing dataset...")
    dataset = dataset.map(create_prompt_formats)#, batched=True) # Keep batched=True
    # Apply preprocessing to each batch of the dataset & and remove 'instruction', 'context', 'response', 'category' fields
    _preprocessing_function = partial(preprocess_batch, max_length=max_length, tokenizer=tokenizer)
    dataset = dataset.map(
        _preprocessing_function,
        batched=True,
        remove_columns=['input', 'output'],
    )
    # Filter out samples that have input_ids exceeding max_length
    dataset = dataset.filter(lambda sample: len(sample["input_ids"]) < max_length)
    # Shuffle dataset
    dataset = dataset.shuffle(seed=seed)
    return dataset

## Pre-process dataset
max_length = get_max_length(original_model)
print(max_length)
train_dataset = preprocess_dataset(tokenizer, max_length,seed, ds['train'])
eval_dataset = preprocess_dataset(tokenizer, max_length,seed, ds['validation'])

"""**9. Preparing the model for QLoRA**"""

#It modifies a quantized model so LoRA
#fine-tuning works without errors, without
#blowing up GPU memory, and without breaking gradients.
from peft import prepare_model_for_kbit_training
original_model = prepare_model_for_kbit_training(original_model)

#Finetuingsegtup
from peft import LoraConfig,get_peft_model
config=LoraConfig(
    r=32, #rank
    lora_alpha=32,
    target_modules=[
                    'q_proj',
                    'k_proj',
                    'v_proj',
                    'dense'
                    ],
    bias="none",
    lora_dropout=0.05,
    task_type="CAUSAL_LM",
)
original_model.gradient_checkpointing_enable()
peft_model=get_peft_model(original_model,config)

#11--Training PEFT adapter
output_dir=f'./peft-dialog-summary-training-{str(int(time.time()))}'
import transformers
peft_training_args=transformers.TrainingArguments(
    output_dir=output_dir,
    warmup_steps=1,
    per_device_train_batch_size=1,
    gradient_accumulation_steps=1, #4
    max_steps=2000,
    learning_rate=2e-4,
    optim="paged_adamw_8bit",
    logging_steps=50,
    logging_dir="./logs",
    save_steps=100,
    eval_steps=100,
    do_eval=True,
    report_to=[],
    gradient_checkpointing=True,
    overwrite_output_dir=True,
    group_by_length=False,
    fp16=True,
)
peft_model.config.use_cache=False
peft_trainer=transformers.Trainer(
    model=peft_model,
    train_dataset=train_dataset,
    eval_dataset=eval_dataset,
    args=peft_training_args,
    data_collator=transformers.DataCollatorForLanguageModeling(tokenizer,mlm=False)
)

peft_trainer.train()

import torch
from transformers import AutoTokenizer,AutoModelForCausalLM
base_model_id="microsoft/phi-2"
base_model=AutoModelForCausalLM.from_pretrained(base_model_id,device_map='auto',quantization_config=bnb_config,trust_remote_code=True,use_auth_token=True)

eval_tokenizer = AutoTokenizer.from_pretrained(base_model_id, add_bos_token=True, trust_remote_code=True, use_fast=False)
eval_tokenizer.pad_token = eval_tokenizer.eos_token

from peft import PeftModel
ft_model = PeftModel.from_pretrained(base_model, f"{output_dir}/checkpoint-2000",torch_dtype=torch.float16,is_trainable=False)

# Commented out IPython magic to ensure Python compatibility.
# %%time
# from transformers import set_seed
# set_seed(seed)
# 
# index = 5
# dialogue = ds['test'][index]['dialogue']
# summary = ds['test'][index]['summary']
# 
# prompt = f"Instruct: Respond supportively and safely to the following message. If distress is implied, encourage seeking help without giving instructions.\n\nUser:\n{dialogue}\n\nResponse:\n"
# 
# 
# peft_model_res = gen(ft_model,prompt,100,)
# peft_model_output = peft_model_res[0].split('Output:\n')[1]
# #print(peft_model_output)
# prefix, success, result = peft_model_output.partition('###')
# 
# dash_line = '-'.join('' for x in range(100))
# print(dash_line)
# print(f'INPUT PROMPT:\n{prompt}')
# print(dash_line)
# print(f'BASELINE HUMAN SUMMARY:\n{summary}\n')
# print(dash_line)
# print(f'PEFT MODEL:\n{prefix}')

#13. Evaluate the Model Quantitatively (with ROUGE Metric)
original_model = AutoModelForCausalLM.from_pretrained(base_model_id,
                                                      device_map='auto',
                                                      quantization_config=bnb_config,
                                                      trust_remote_code=True,
                                                      use_auth_token=True)

import pandas as pd

dialogues_input = ds['test'][0:10]['input'] # Changed to 'input'
human_baseline_outputs = ds['test'][0:10]['output'] # Changed to 'output'

original_model_summaries = []
peft_model_summaries = []

for idx, dialogue in enumerate(dialogues_input):
    human_baseline_text_output = human_baseline_outputs[idx]
    prompt = f"Instruct: Summarize the following conversation.\n{dialogue}\nOutput:\n"

    original_model_res = gen(original_model,prompt,100,)
    original_model_text_output = original_model_res[0].split('Output:\n')[1]

    peft_model_res = gen(ft_model,prompt,100,)
    peft_model_output_full = peft_model_res[0].split('Output:\n')[1]
    # The partition by '###' is not needed here as it's not present in the output of gen for this prompt format.
    # Just take the full output after 'Output:\n'
    peft_model_text_output = peft_model_output_full

    original_model_summaries.append(original_model_text_output)
    peft_model_summaries.append(peft_model_text_output)

zipped_summaries = list(zip(human_baseline_outputs, original_model_summaries, peft_model_summaries))

df = pd.DataFrame(zipped_summaries, columns = ['human_baseline_op', 'original_model_op', 'peft_model_op'])
df

import evaluate

rouge = evaluate.load('rouge')

original_model_results = rouge.compute(
    predictions=original_model_summaries,
    references=human_baseline_outputs[0:len(original_model_summaries)],
    use_aggregator=True,
    use_stemmer=True,
)

peft_model_results = rouge.compute(
    predictions=peft_model_summaries,
    references=human_baseline_outputs[0:len(peft_model_summaries)],
    use_aggregator=True,
    use_stemmer=True,
)

print('ORIGINAL MODEL:')
print(original_model_results)
print('PEFT MODEL:')
print(peft_model_results)

print("Absolute percentage improvement of PEFT MODEL over ORIGINAL MODEL")

improvement = (np.array(list(peft_model_results.values())) - np.array(list(original_model_results.values())))
for key, value in zip(peft_model_results.keys(), improvement):
    print(f'{key}: {value*100:.2f}%')

import torch
def generate_response(model, tokenizer, prompt, max_new_tokens=150):
    inputs = tokenizer(prompt, return_tensors="pt").to(model.device)

    with torch.no_grad():
        outputs = model.generate(
            **inputs,
            max_new_tokens=max_new_tokens,
            do_sample=True,
            temperature=0.7,
            top_p=0.9
        )

    return tokenizer.decode(outputs[0], skip_special_tokens=True)

def build_prompt(user_text):
    return f"""
Instruct: You are a supportive mental-health conversation assistant.
Respond with empathy and understanding.
Do NOT diagnose or give medical advice.

User:
{user_text}

Response:
"""

test_inputs = [
    "I feel exhausted all the time and nothing I do seems to help.",
    "I feel ashamed for feeling anxious all the time.",
    "Sometimes I feel like disappearing because I’m tired of everything.",
    "I haven’t been sleeping well and it’s affecting my work.",
    "Do you think I have depression?"
]

for i, text in enumerate(test_inputs, 1):
    prompt = build_prompt(text)

    base_out = generate_response(base_model, tokenizer, prompt)
    peft_out = generate_response(ft_model, tokenizer, prompt)

    print("="*80)
    print(f"Example {i}")
    print("-"*80)
    print("User Input:")
    print(text)

    print("\nBase Phi-2 Output:")
    print(base_out)

    print("\nPEFT Phi-2 Output:")
    print(peft_out)

with open("qualitative_results.txt", "w") as f:
    for i, text in enumerate(test_inputs, 1):
        prompt = build_prompt(text)

        base_out = generate_response(base_model, tokenizer, prompt)
        peft_out = generate_response(ft_model, tokenizer, prompt)

        f.write(f"Example {i}\n")
        f.write(f"User: {text}\n\n")
        f.write("Base Phi-2:\n")
        f.write(base_out + "\n\n")
        f.write("PEFT Phi-2:\n")
        f.write(peft_out + "\n")
        f.write("="*80 + "\n\n")